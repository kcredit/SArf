---
title: "Getting Started with SArf"
author: "Kevin Credit"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with SArf}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Introduction

This vignette demonstrates how to use the SArf package for spatial
autoregressive random forest analysis from start to finish.

# Installation

## One-Time Setup

```{r install}
# Install development tools
install.packages("devtools")
install.packages("roxygen2")

# Install SArf dependencies
install.packages(c(
  "ranger", "sf", "spdep", "spatialreg", "blockCV", "ALEPlot",
  "ggplot2", "dplyr", "tidyr", "leaflet", "htmltools"
))

# Install SArf from GitHub
devtools::install_github("kcredit/SArf", force = TRUE)

# Or from local directory
# setwd("~/path/to/SArf")
# devtools::document()
# devtools::install()

# IMPORTANT: Restart R session after installation!
```

# Load Libraries

```{r libraries}
library(sf)
library(dplyr)
library(SArf)

# Note: Only load these three to avoid conflicts!
```

# Real Example: Health Rating Index for Dublin

This example uses the full dataset from the Health Rating Index project.

**Project Background:** SArf was developed as part of research analyzing
environmental health burdens and benefits across Dublin, Ireland. The
full project is available at
<https://github.com/kcredit/health-rating-index>

## Load Data

```{r dublin_example}
# Load included data
data_path <- system.file("extdata", "model_data.shp", package = "SArf")
data <- st_read(data_path)

# Check the data
cat("Dataset size:", nrow(data), "small areas\n")
summary(data$HRI_gaus_n)

# Variables included:
# - HRI_gaus_n: Health Rating Index (outcome)
# - In22_ED: Deprivation index
# - NoAuto_p: % households without car
# - POPD: Population density
# - log_dist: Log distance to city center
# - ov60: % over 60 years old
# - nonIrish: % non-Irish nationals

# Clean the data
data_clean <- data %>%
  # Remove missing outcome
  filter(!is.na(HRI_gaus_n)) %>%
  # Ensure numeric
  mutate(your_outcome_variable = as.numeric(HRI_gaus_n)) %>%
  # Remove missing predictors (add all your predictor names)
  filter(complete.cases(HRI_gaus_n, In22_ED, NoAuto_p, POPD, log_dist, ov60, nonIrish)) %>%
  # Fix invalid geometries
  filter(st_is_valid(.))

# Fix duplicate coordinates (important!)
st_geometry(data_clean) <- st_jitter(st_geometry(data_clean), amount = 0.00001)

# Transform to projected CRS (required for spatial blocking)
data_clean <- st_transform(data_clean, 3857)

# Check data
cat("Cleaned data:", nrow(data_clean), "rows\n")
summary(data_clean$HRI_gaus_p)

cat("Any NAs?", sum(is.na(data_clean$HRI_gaus_p)), "\n")
```

## Calculate Parameters

```{r calc_params}
n_rows <- nrow(data_clean)
cat("Dataset size:", n_rows, "rows\n")

# Choose parameters based on size
if (n_rows < 1000) {
  # Small dataset - full analysis
  n_folds <- 5
  n_bootstrap <- 20
  num_trees <- 500
  cat("Using full analysis parameters\n")
  
} else if (n_rows < 3000) {
  # Medium dataset - moderate
  n_folds <- 5
  n_bootstrap <- 10
  num_trees <- 500
  cat("Using moderate analysis parameters\n")
  
} else {
  # Very large - sample first
  cat("Very large dataset. Sampling 3000 rows...\n")
  set.seed(1111)
  data <- slice_sample(data_clean, n = 3000)
  n_folds <- 3
  n_bootstrap <- 5
  num_trees <- 250
}

cat("If your data are very large, you can alternatively (or additionally) increase the block range (rough guides):
    Neighbourhood: 500-1000m
    City: 5000-10000m
    County: 12500-25000m
    Region/Country: 50000-100000m", "\n")

# Get bounding box of your data
bbox <- st_bbox(data)

# Calculate width and height (in meters if projected)
width <- bbox["xmax"] - bbox["xmin"]
height <- bbox["ymax"] - bbox["ymin"]

# Use smaller dimension (more conservative)
min_dimension <- min(width, height)/3

```

## Run Analysis

```{r run_dublin}
# Run SArf analysis
results <- SArf(
  HRI_gaus_n ~ In22_ED + NoAuto_p + POPD + log_dist + ov60 + nonIrish,
  data = data,
  k_neighbors = 8,             # Number of neighbors for spatial weights
  n_folds = n_folds,           # Spatial CV folds
  n_bootstrap = n_bootstrap,   # Bootstrap iterations
  num_trees = num_trees,       # Trees in random forest
  include_naive_rf = TRUE,     # For comparison with "naive" random forest
  naive_test_fraction = 0.2,   
  create_map = TRUE,           # Spatial folds map
  block_range = min_dimension, # Size of the spatial blocks in units of metres
  verbose = TRUE               # Print progress
)
```

## View Results

```{r view_dublin_results}
# Print all results (not recommended)
# print(results)

# Model comparison
results$model_comparison

# Variable importance
results$importance_plot

# ALE plots
results$ale_plots

# Full sptial econometric model results
show_models(results)

# Interactive map
results$leaflet_map
```

## Save Outputs

```{r save_outputs}
# Create output directory
dir.create("output", showWarnings = FALSE)

# Save plots
ggsave("output/moran_plot.png", results$moran_plot, width = 6, height = 5)
ggsave("output/importance_plot.png", results$importance_plot, width = 8, height = 6)
ggsave("output/ale_plots.png", results$ale_plots, width = 10, height = 8)

# Save tables
write.csv(results$model_comparison, "output/model_comparison.csv", row.names = FALSE)
write.csv(results$variable_importance, "output/variable_importance.csv", row.names = FALSE)
write.csv(results$ale_results, "output/ale_data.csv", row.names = FALSE)

# Save map
# library(htmlwidgets)
# saveWidget(results$leaflet_map, "output/results_map.html")

cat("All outputs saved to output/ directory\n")
```

## Verify Results

```{r verify_results}
# =================================================================
# VERIFICATION CODE TO ENSURE SPATIAL LAGS ARE CALCULATED CORRECTLY
# =================================================================

# Extract spatial_weights from results
spatial_weights <- results$spatial_weights

# ========================================
# 1. Get fold structure for verification
# ========================================
fold_1 <- results$spatial_cv_results$predictions %>%
  filter(fold == 1, iteration == 1)

test_idx <- fold_1 %>% filter(in_training == FALSE) %>% pull(row_id)
train_idx <- fold_1 %>% filter(in_training == TRUE) %>% pull(row_id)

cat("Fold 1, Iteration 1:\n")
cat("Training observations:", length(train_idx), "\n")
cat("Test observations:", length(test_idx), "\n\n")

# ========================================
# 2. Get coordinates and extract k
# ========================================
all_coords <- st_coordinates(st_centroid(st_geometry(data)))
train_coords <- all_coords[train_idx, , drop = FALSE]

# Extract k from spatial_weights
k_neighbors <- round(mean(sapply(spatial_weights$neighbours, length)))
cat("k (number of neighbors):", k_neighbors, "\n\n")

# ========================================
# 3. For EACH test observation, verify neighbors are ALL from training set
# ========================================
cat("=== Verifying Spatial Lag Calculation ===\n\n")

# Get response variable name from results
response_var <- all.vars(results$formula)[1]

verification_results <- data.frame(
  test_obs = test_idx,
  stored_lag = numeric(length(test_idx)),
  calculated_lag = numeric(length(test_idx)),
  match = logical(length(test_idx)),
  n_neighbors = numeric(length(test_idx)),
  any_from_test = logical(length(test_idx))
)

for (i in 1:length(test_idx)) {
  obs <- test_idx[i]
  
  # Get stored spatial lag from predictions
  stored_lag <- fold_1 %>% filter(row_id == obs) %>% pull(spatial_lag)
  
  # Calculate what spatial lag SHOULD be (using training neighbors only)
  obs_coord <- all_coords[obs, , drop = FALSE]
  
  # Find distances to ALL training observations
  dists <- sqrt(rowSums(sweep(train_coords, 2, obs_coord, "-")^2))
  
  # Find k nearest TRAINING neighbors
  k_to_use <- min(k_neighbors, length(dists))
  nearest_indices <- order(dists)[1:k_to_use]
  neighbor_rows <- train_idx[nearest_indices]
  
  # Calculate spatial lag with W-style weights (simple mean)
  calculated_lag <- mean(data[[response_var]][neighbor_rows])
  
  # Check if any neighbors are from test set (should be NONE!)
  any_from_test <- any(neighbor_rows %in% test_idx)
  
  # Store results
  verification_results$stored_lag[i] <- stored_lag
  verification_results$calculated_lag[i] <- calculated_lag
  verification_results$match[i] <- abs(stored_lag - calculated_lag) < 0.001
  verification_results$n_neighbors[i] <- k_to_use
  verification_results$any_from_test[i] <- any_from_test
}

# ========================================
# 4. Summary Statistics
# ========================================
cat("=== VERIFICATION RESULTS ===\n\n")

cat("Total test observations checked:", nrow(verification_results), "\n")
cat("Stored values match calculated:", sum(verification_results$match), "/", nrow(verification_results), "\n")
cat("Observations with neighbors from test set:", sum(verification_results$any_from_test), "\n\n")

if (sum(verification_results$any_from_test) > 0) {
  cat("❌ PROBLEM: Some test observations have neighbors from test set!\n")
  cat("This indicates data leakage.\n\n")
  
  # Show which observations have test neighbors
  problem_obs <- verification_results %>% filter(any_from_test == TRUE)
  cat("Problem observations:\n")
  print(problem_obs %>% select(test_obs, any_from_test))
  
} else {
  cat("✅ SUCCESS: NO test observations have neighbors from test set!\n")
  cat("All spatial lags use only training neighbors.\n\n")
}

if (all(verification_results$match)) {
  cat("✅ SUCCESS: All stored spatial_lag values match manual calculations!\n")
  cat("The order() bug is fixed.\n\n")
} else {
  cat("❌ PROBLEM: Some stored values don't match calculations!\n")
  cat("Number of mismatches:", sum(!verification_results$match), "\n\n")
  
  # Show mismatches
  mismatches <- verification_results %>% filter(match == FALSE)
  cat("Mismatched observations (first 10):\n")
  print(head(mismatches %>% select(test_obs, stored_lag, calculated_lag), 10))
}

# ========================================
# 5. Detailed Check for Random Sample
# ========================================
cat("\n=== DETAILED CHECK: 5 Random Test Observations ===\n\n")

sample_obs <- sample(test_idx, min(5, length(test_idx)))

for (obs in sample_obs) {
  cat("Observation:", obs, "\n")
  
  # Get stored value
  stored <- fold_1 %>% filter(row_id == obs) %>% pull(spatial_lag)
  
  # Calculate manually
  obs_coord <- all_coords[obs, , drop = FALSE]
  dists <- sqrt(rowSums(sweep(train_coords, 2, obs_coord, "-")^2))
  k_to_use <- min(k_neighbors, length(dists))
  nearest_indices <- order(dists)[1:k_to_use]
  neighbor_rows <- train_idx[nearest_indices]
  calculated <- mean(data[[response_var]][neighbor_rows])
  
  cat("  Stored spatial_lag:    ", round(stored, 6), "\n")
  cat("  Calculated spatial_lag:", round(calculated, 6), "\n")
  cat("  Match:                 ", abs(stored - calculated) < 0.001, "\n")
  cat("  Number of neighbors:   ", k_to_use, "\n")
  cat("  All from training set: ", all(neighbor_rows %in% train_idx), "\n")
  cat("  Any from test set:     ", any(neighbor_rows %in% test_idx), "\n")
  
  # Show neighbor IDs
  cat("  Neighbor row IDs:      ", paste(head(neighbor_rows, 10), collapse = ", "))
  if (k_to_use > 10) cat(", ...")
  cat("\n\n")
}

# ========================================
# 6. Final Summary
# ========================================
cat("=== FINAL ASSESSMENT ===\n\n")

if (sum(verification_results$any_from_test) == 0 && all(verification_results$match)) {
  cat("✅✅✅ PERFECT! ✅✅✅\n")
  cat("1. NO data leakage (all neighbors from training set)\n")
  cat("2. Storage is correct (stored values match calculations)\n")
  cat("3. Your spatial CV implementation is working correctly!\n")
} else {
  if (sum(verification_results$any_from_test) > 0) {
    cat("❌ Data leakage detected\n")
  }
  if (!all(verification_results$match)) {
    cat("❌ Storage bug detected\n")
  }
  cat("\nSee details above for specific issues.\n")
}

```

## Full Project

For the complete analysis and reproducible code: - **GitHub:**
<https://github.com/kcredit/health-rating-index> - **Publication:**
Credit et al. (2025), GISRUK Conference Proceedings - **DOI:**
10.5281/zenodo.15183740

# Interpretation Guide

## Model Comparison

-   **RMSE**: Lower is better
-   **R²**: Higher is better (0-1 scale)
-   **Moran's I**: Close to 0 indicates no spatial autocorrelation in
    residuals
-   **Typically**: RF \> spatial models for R², spatial models remove
    autocorrelation

## Variable Importance

-   High mean = strong predictor
-   Narrow CI = consistent across folds
-   spatial_lag often important (neighborhood effects)

## ALE Plots

-   Positive slope = positive effect
-   Negative slope = negative effect
-   Curves = non-linear relationships
-   Confidence bands = uncertainty

# Tips and Best Practices

**Dataset Size:** - \< 1000: Full parameters - 1000-3000: Moderate
parameters - 3000: Sample or reduce parameters

**Block Range:** - Too small → "same area" error - Too large → empty
folds - Start with min(width, height) / 3

**Performance:** - 500 obs: \~5 min - 1000 obs: \~10 min - 2000 obs:
\~20 min - 3000 obs: \~45 min

**Avoid Conflicts:** - Only load: SArf, sf, dplyr - Don't load ranger,
spdep, etc. directly

# Troubleshooting

**"All points from same area"** - Increase block_range - Reduce n_folds

**Moran's I shows NA** - Check for missing values - Ensure no duplicate
coordinates

**Very slow** - Sample to 2000-3000 rows - Reduce n_bootstrap to 3-5

# Citation

```         
Credit, K. (2025). SArf: Spatial Autoregressive 
Random Forest. https://github.com/YOUR-USERNAME/SArf

Credit, K., Kumar, D., & Eccles, E. (2025). Exploring the 
transport-health-environment nexus through a new 'Health Rating Index'. 
Proceedings of the 33rd GISRUK Conference. DOI: 10.5281/zenodo.15183740
```

# Session Info

```{r sessioninfo}
sessionInfo()
```
